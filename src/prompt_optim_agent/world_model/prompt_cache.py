"""
Sistema de cache de avalia√ß√£o de prompts baseado em similaridade sem√¢ntica.

Economiza tempo pulando avalia√ß√µes de prompts muito similares,
com thresholds adaptativos baseados no momentum e acur√°cia.

ABORDAGEM SEM√ÇNTICA:
- Usa embeddings (SentenceTransformers) para capturar significado
- Calcula cosine similarity entre embeddings de prompts
- Dist√¢ncia sem√¢ntica = 1 - cosine_similarity
- Mais robusto que substring matching (difflib)

EXEMPLO DE USO:
    parent_prompt = "Classify the sentiment of the following text"
    candidate_prompt = "Determine if the text below is positive or negative"
    
    # Gerar embeddings
    parent_embedding = model.encode(parent_prompt)
    candidate_embedding = model.encode(candidate_prompt)
    
    # Calcular similaridade sem√¢ntica
    cos_sim = util.pytorch_cos_sim(candidate_embedding, parent_embedding).item()
    # cos_sim ‚âà 0.85 (alto, pois prompts t√™m mesmo significado)
    
    # Dist√¢ncia sem√¢ntica
    dist_to_parent = 1 - cos_sim  # ‚âà 0.15 (baixo)
    
    # Decis√£o: Se cos_sim >= threshold ‚Üí usar cache
    if cos_sim >= 0.85:  # threshold para HARD+low_acc
        return cached_accuracy
"""

import hashlib
import numpy as np
from typing import Dict, Optional, Tuple
from collections import OrderedDict
from sentence_transformers import SentenceTransformer, util
import torch


class PromptEvaluationCache:
    """
    Cache inteligente para avalia√ß√µes de prompts.
    
    Features:
    - Compara√ß√£o exata por hash
    - Compara√ß√£o sem√¢ntica via embeddings (cosine similarity)
    - Thresholds adaptativos por momentum e acur√°cia
    - Limite de tamanho (LRU)
    """
    
    def __init__(
        self,
        logger,
        enable_cache: bool = True,
        max_cache_size: int = 100,
        high_acc_threshold: float = 0.70,
        embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'
    ):
        """
        Args:
            logger: Logger para output
            enable_cache: Habilita/desabilita cache
            max_cache_size: N√∫mero m√°ximo de entradas no cache
            high_acc_threshold: Threshold para considerar "alta acur√°cia"
            embedding_model: Nome do modelo de embeddings para similaridade sem√¢ntica
        """
        self.logger = logger
        self.enable_cache = enable_cache
        self.max_cache_size = max_cache_size
        self.high_acc_threshold = high_acc_threshold
        
        # Inicializar modelo de embeddings
        self.logger.info(f"üîß Carregando modelo de embeddings: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.logger.info("‚úÖ Modelo de embeddings carregado com sucesso")
        
        # Cache: {prompt_hash: {
        #   'prompt': str, 
        #   'accuracy': float, 
        #   'momentum': str,
        #   'embedding': tensor
        # }}
        self.cache = OrderedDict()
        
        # Thresholds de similaridade adaptativos (baseado em cosine similarity)
        self.similarity_thresholds = {
            'HARD': {
                'high_acc': 0.98,   # Rigoroso: pequenas mudan√ßas importam
                'low_acc': 0.85     # Relaxado: precisa mudan√ßas grandes
            },
            'MEDIUM': 0.95,
            'EASY': 0.93
        }
        
        # Estat√≠sticas
        self.stats = {
            'exact_hits': 0,
            'similarity_hits': 0,
            'misses': 0,
            'evaluations_saved': 0
        }
    
    def _hash_prompt(self, prompt: str) -> str:
        """Gera hash MD5 do prompt."""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
    
    def _get_embedding(self, prompt: str) -> torch.Tensor:
        """
        Gera embedding sem√¢ntico do prompt.
        
        Args:
            prompt: Texto do prompt
            
        Returns:
            Tensor com embedding do prompt
        """
        with torch.no_grad():
            embedding = self.embedding_model.encode(
                prompt, 
                convert_to_tensor=True,
                show_progress_bar=False
            )
        return embedding
    
    def _calculate_similarity(self, prompt1: str, prompt2: str) -> float:
        """
        Calcula similaridade sem√¢ntica entre dois prompts usando cosine similarity.
        
        Abordagem sem√¢ntica (embeddings) ao inv√©s de substring matching.
        Isso captura significado, n√£o apenas caracteres.
        
        Args:
            prompt1: Primeiro prompt
            prompt2: Segundo prompt
            
        Returns:
            Cosine similarity entre 0.0 e 1.0 (1.0 = semanticamente id√™nticos)
        """
        if prompt1 == prompt2:
            return 1.0
        
        # Gerar embeddings
        embedding1 = self._get_embedding(prompt1)
        embedding2 = self._get_embedding(prompt2)
        
        # Calcular cosine similarity
        cos_sim = util.pytorch_cos_sim(embedding1, embedding2).item()
        
        # Converter para similaridade (0-1, onde 1 = id√™ntico)
        # Cosine similarity j√° retorna valores entre -1 e 1, 
        # mas para embeddings de texto geralmente fica entre 0 e 1
        similarity = max(0.0, cos_sim)  # Garante n√£o-negativo
        
        return similarity
    
    def _calculate_semantic_distance(self, prompt1: str, prompt2: str) -> float:
        """
        Calcula dist√¢ncia sem√¢ntica entre dois prompts.
        
        Dist√¢ncia = 1 - similaridade
        
        Args:
            prompt1: Primeiro prompt
            prompt2: Segundo prompt
            
        Returns:
            Dist√¢ncia sem√¢ntica entre 0.0 e 1.0 (0.0 = id√™nticos)
        """
        similarity = self._calculate_similarity(prompt1, prompt2)
        distance = 1.0 - similarity
        return distance
    
    def _get_similarity_threshold(self, momentum: str, current_acc: float) -> float:
        """
        Retorna threshold de similaridade adaptativo.
        
        Estrat√©gia:
        - HARD + Alta acur√°cia (>70%): Threshold alto (0.98)
          ‚Üí Pequenas mudan√ßas podem fazer diferen√ßa em exemplos dif√≠ceis
        - HARD + Baixa acur√°cia (<70%): Threshold baixo (0.85)
          ‚Üí Precisa de mudan√ßas significativas para sair do m√≠nimo local
        - MEDIUM/EASY: Thresholds fixos
        
        Args:
            momentum: N√≠vel de dificuldade atual ('EASY', 'MEDIUM', 'HARD')
            current_acc: Acur√°cia atual (0.0 a 1.0)
            
        Returns:
            Threshold de similaridade (0.0 a 1.0)
        """
        if momentum == 'HARD':
            if current_acc > self.high_acc_threshold:
                threshold = self.similarity_thresholds['HARD']['high_acc']
                self.logger.debug(
                    f"HARD + High Acc ({current_acc:.1%}) ‚Üí "
                    f"Strict threshold: {threshold}"
                )
            else:
                threshold = self.similarity_thresholds['HARD']['low_acc']
                self.logger.debug(
                    f"HARD + Low Acc ({current_acc:.1%}) ‚Üí "
                    f"Relaxed threshold: {threshold}"
                )
        else:
            threshold = self.similarity_thresholds[momentum]
            self.logger.debug(f"{momentum} ‚Üí Threshold: {threshold}")
        
        return threshold
    
    def get(
        self, 
        prompt: str, 
        momentum: str = 'MEDIUM',
        current_acc: float = 0.5
    ) -> Optional[float]:
        """
        Busca avalia√ß√£o no cache comparando com TODOS os prompts armazenados.
        
        Usa busca vetorizada para encontrar o prompt mais similar em todo o cache.
        
        Args:
            prompt: Prompt a buscar
            momentum: Momentum atual para threshold adaptativo
            current_acc: Acur√°cia atual para threshold adaptativo
            
        Returns:
            Acur√°cia do cache se encontrado, None caso contr√°rio
        """
        if not self.enable_cache:
            return None
        
        prompt_hash = self._hash_prompt(prompt)
        
        # 1. Tenta match exato
        if prompt_hash in self.cache:
            cached_data = self.cache[prompt_hash]
            self.stats['exact_hits'] += 1
            self.logger.info(
                f" Cache EXACT HIT! "
                f"Acc: {cached_data['accuracy']:.2%}"
            )
            # Move para o final (LRU)
            self.cache.move_to_end(prompt_hash)
            return cached_data['accuracy']
        
        # 2. Busca por similaridade sem√¢ntica em TODOS os prompts do cache
        if len(self.cache) > 0:
            threshold = self._get_similarity_threshold(momentum, current_acc)
            
            # Gerar embedding do prompt atual UMA VEZ
            current_embedding = self._get_embedding(prompt)
            
            # Coletar todos os embeddings e dados do cache
            cache_embeddings = []
            cache_data_list = []
            cache_hashes = []
            
            for cache_hash, cached_data in self.cache.items():
                cache_embeddings.append(cached_data['embedding'])
                cache_data_list.append(cached_data)
                cache_hashes.append(cache_hash)
            
            # Stack embeddings em um tensor 2D para busca vetorizada
            cache_embeddings_tensor = torch.stack(cache_embeddings)
            
            # Calcula similaridade com TODOS os prompts de uma vez (vetorizado)
            similarities = util.pytorch_cos_sim(current_embedding, cache_embeddings_tensor)[0]
            
            # Encontra o √≠ndice com maior similaridade
            best_idx = similarities.argmax().item()
            best_similarity = similarities[best_idx].item()
            best_match = cache_data_list[best_idx]
            best_hash = cache_hashes[best_idx]
            
            dist_to_best = 1 - best_similarity
            
            self.logger.debug(
                f" Compared with {len(self.cache)} prompts in cache "
                f"(vectorized search)"
            )
            
            # Verifica se o melhor match passa do threshold
            if best_similarity >= threshold:
                self.stats['similarity_hits'] += 1
                self.stats['evaluations_saved'] += 1
                
                self.logger.info(
                    f"   Cache SEMANTIC HIT! Found best match:\n"
                    f"   Cosine Sim: {best_similarity:.4f} >= {threshold:.3f}\n"
                    f"   Distance: {dist_to_best:.4f}\n"
                    f"   Matched prompt: '{best_match['prompt'][:70]}...'\n"
                    f"   Matched momentum: {best_match['momentum']}\n"
                    f"   Reusing acc: {best_match['accuracy']:.2%}"
                )
                
                # Adiciona entrada com hash do novo prompt apontando para mesma acc
                self.put(prompt, best_match['accuracy'], momentum, current_embedding)
                
                # Move o match usado para o final (mais recente no LRU)
                self.cache.move_to_end(best_hash)
                
                return best_match['accuracy']
            else:
                self.logger.info(
                    f"Cache MISS. Best match not similar enough:\n"
                    f"   Best Cosine Sim: {best_similarity:.4f} < {threshold:.3f}\n"
                    f"   Distance: {dist_to_best:.4f}"
                )
        
        # Cache miss
        self.stats['misses'] += 1
        return None
    
    def put(
        self, 
        prompt: str, 
        accuracy: float, 
        momentum: str = 'MEDIUM',
        embedding: Optional[torch.Tensor] = None
    ):
        """
        Adiciona avalia√ß√£o ao cache com seu embedding sem√¢ntico.
        
        Args:
            prompt: Prompt avaliado
            accuracy: Acur√°cia obtida
            momentum: Momentum usado na avalia√ß√£o
            embedding: Embedding pr√©-calculado (opcional, ser√° gerado se None)
        """
        if not self.enable_cache:
            return
        
        prompt_hash = self._hash_prompt(prompt)
        
        # Gerar embedding se n√£o fornecido
        if embedding is None:
            embedding = self._get_embedding(prompt)
        
        # Adiciona ao cache
        self.cache[prompt_hash] = {
            'prompt': prompt,
            'accuracy': accuracy,
            'momentum': momentum,
            'embedding': embedding
        }
        
        # Move para o final (mais recente)
        self.cache.move_to_end(prompt_hash)
        
        # Remove entradas antigas se exceder tamanho
        while len(self.cache) > self.max_cache_size:
            oldest_hash = next(iter(self.cache))
            removed = self.cache.pop(oldest_hash)
            self.logger.debug(
                f"üóëÔ∏è Cache cheio - removida a entrada mais antiga "
                f"(acc: {removed['accuracy']:.2%})"
            )
        
        self.logger.info(
            f"Cached evaluation - Acur√°cia Registrada: {accuracy:.2%} "
            f"(cache size: {len(self.cache)}/{self.max_cache_size})"
        )
    
    def clear(self):
        """Limpa o cache."""
        self.cache.clear()
        self.logger.info("Cache cleared")
    
    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas do cache."""
        total_queries = sum([
            self.stats['exact_hits'],
            self.stats['similarity_hits'],
            self.stats['misses']
        ])
        
        hit_rate = 0.0
        if total_queries > 0:
            hit_rate = (self.stats['exact_hits'] + self.stats['similarity_hits']) / total_queries
        
        return {
            **self.stats,
            'total_queries': total_queries,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        }
    
    def log_stats(self):
        """Loga estat√≠sticas do cache."""
        stats = self.get_stats()
        
        self.logger.info("\n" + "="*60)
        self.logger.info("CACHE STATISTICS")
        self.logger.info("="*60)
        self.logger.info(f"Total queries:        {stats['total_queries']}")
        self.logger.info(f"Exact hits:           {stats['exact_hits']}")
        self.logger.info(f"Similarity hits:      {stats['similarity_hits']}")
        self.logger.info(f"Misses:               {stats['misses']}")
        self.logger.info(f"Hit rate:             {stats['hit_rate']:.1%}")
        self.logger.info(f"Evaluations saved:    {stats['evaluations_saved']}")
        self.logger.info(f"Cache size:           {stats['cache_size']}/{self.max_cache_size}")
        self.logger.info("="*60 + "\n")
